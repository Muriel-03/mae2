{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import datasets # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
    "import es_core_news_sm # Modelo Spacy de procesamiento de lenguaje natural en español\n",
    "import spacy # Biblioteca de procesamiento de lenguaje natural\n",
    "import pandas as pd # Biblioteca de manejo de conjuntos de datos\n",
    "import re # Módulo de expresiones regulares\n",
    "import tokenizers # Biblioteca de tokenización de texto\n",
    "import nltk # Biblioteca de procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤗 Datasets\n",
    "\n",
    "🤗 (HuggingFace) Datasets es una biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural que se destaca por la simplicidad de sus métodos y el gran repositorio 🤗 Hub que contiene muchos conjuntos de datos libres para descargar sólo con una linea de Python.\n",
    "\n",
    "En nuestro curso trabajaremos con `spanish_diagnostics`, un conjunto de datos de nuestro grupo investigación PLN@CMM que contiene textos de sospechas diagnósticas de la lista de espera chilena y está etiquetado con el destino de la interconsulta; este destino puede ser `dental` o `no_dental`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset spanish_diagnostics (C:\\Users\\ville\\.cache\\huggingface\\datasets\\spanish_diagnostics\\default\\0.0.0\\45c176cea64580ea9631f78c2867a657ede368597681e5337e9f1c976e4e84ff)\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics = datasets.load_dataset('fvillena/spanish_diagnostics') # Con esta linea descargamos el conjunto de datos completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro conjunto de datos cuenta con 2 particiones, una partición `train` y otra `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase utilizaremos la partición `train` del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder facilmente a atributos de nuestro `Dataset`.\n",
    "\n",
    "- `shape`: Tal como en muchas otras bibliotecas de python este atributo contiene la forma de nuestro conjunto de datos con la sintaxis `(filas, columnas)`.\n",
    "- `column_names`: Este atributo contiene el nombre de las características que tiene nuestro conjunto de datos. En nuestro caso tenemos una característica `text`, la cual contiene la hipótesis diagnóstica del conjunto de datos y `label` que contiene el destino al cual fue referido.\n",
    "- `features`: Este atributo nos describe la clase a la que pertenece cada una de las características. En nuestro caso `text` es un `string` y `label` es del tipo `ClassLabel` con 2 clases con nombre `not_dental` y `dental`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_dental', 'dental'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como en muchas otras clases de datos en Python podemos acceder a subconjuntos de datos a través de sus índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': '- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 0, 0],\n",
       " 'text': ['- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       "  'OBTRUCCION FOSA NASAL DERECHA',\n",
       "  'Perturbación de la actividad y de la atención Trastorno defícit atencional']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 1],\n",
       " 'text': ['OBTRUCCION FOSA NASAL DERECHA',\n",
       "  'M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 ALGIA PELVICA HTA CRONICA',\n",
       "  'pieza n 3.4 tratada endodonticamente, restaurada con ionomero y resina compuesta. Necesita protesis fija por gran pNrdida coronaria']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][1,3,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos acceder a cada una de las características por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'OBTRUCCION FOSA NASAL DERECHA',\n",
       " 'Perturbación de la actividad y de la atención Trastorno defícit atencional']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"]['text'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 🤗 Datasets también podemos trabajar en otras bibliotecas, como por ejemplo importar el conjunto de datos en Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_diagnostics_train_df = pd.DataFrame(spanish_diagnostics[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>OBTRUCCION FOSA NASAL DERECHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Perturbación de la actividad y de la atención ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>1</td>\n",
       "      <td>DM1 Evaluación</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>1</td>\n",
       "      <td>ABCESO SUBMUCOSO PIEZA 2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0</td>\n",
       "      <td>Pbs Inmunodeficiencia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0</td>\n",
       "      <td>QUISTE SINOVIAL DEL HUECO POPLITEO, DE BAKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0</td>\n",
       "      <td>MALLET FINGE D 1 MANO DERECHA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          1  - ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUS...\n",
       "1          0                      OBTRUCCION FOSA NASAL DERECHA\n",
       "2          0  Perturbación de la actividad y de la atención ...\n",
       "3          0  M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 A...\n",
       "4          1  PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A C...\n",
       "...      ...                                                ...\n",
       "69995      1                                     DM1 Evaluación\n",
       "69996      1                         ABCESO SUBMUCOSO PIEZA 2.6\n",
       "69997      0                              Pbs Inmunodeficiencia\n",
       "69998      0       QUISTE SINOVIAL DEL HUECO POPLITEO, DE BAKER\n",
       "69999      0                      MALLET FINGE D 1 MANO DERECHA\n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que nuestro conjunto de datos tiene sus clases balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    35034\n",
       "0    34966\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos localmente un conjunto de datos y queremos importarlo a 🤗 Datasets también podemos hacerlo. Aquí importamos el conjunto de datos desde un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-68d4ea5b370697bf\n",
      "Reusing dataset csv (C:\\Users\\ville\\.cache\\huggingface\\datasets\\csv\\default-68d4ea5b370697bf\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['diagnostic', 'is_dental'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_dataset('csv', data_files='spanish_diagnostics/spanish_diagnostics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "\n",
    "Una de las tareas que podemos realizar sobre las características no estructuradas de texto es la normalización. La cual consiste en llevar nuestro texto a una forma más consistente a lo largo del conjunto de datos.\n",
    "\n",
    "Podemos observar que nuestro conjunto de datos cuenta con una alta inconsistencia respecto al uso de mayúsculas, el uso de tildes y el uso de signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'OBTRUCCION FOSA NASAL DERECHA',\n",
       " 'Perturbación de la actividad y de la atención Trastorno defícit atencional',\n",
       " 'M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 ALGIA PELVICA HTA CRONICA',\n",
       " 'PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A CAMARA PULPAR, EVALUAR POR ESPECIALIDAD',\n",
       " 'pieza n 3.4 tratada endodonticamente, restaurada con ionomero y resina compuesta. Necesita protesis fija por gran pNrdida coronaria',\n",
       " 'PZ. 12 TREPANADA',\n",
       " 'CARCINOMA TORIODEO',\n",
       " 'DISPEPSIA Y METEORISMO',\n",
       " 'ASA 1 DENTICION TEMPORAL MORDIDA CRUZADA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][\"text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder llevar todo a minúsculas, simplemente podemos utilizar el método str.lower()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- anomalías dentofaciales (incluso la maloclusión)\\n\\n\\n discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence_lower = spanish_diagnostics[\"train\"][\"text\"][0].lower()\n",
    "sample_sentence_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar todo los caracteres no alfabéticos podemos utilizar un patrón de expresión regular con la sintaxis: `[^a-zñáéíóú]`, la cual se explica como:\n",
    "\n",
    "- `^`: Este es un `NO` lógico que invierte todo lo que viene a su derecha.\n",
    "- `a-z`: Este patrón coincide todos los caracteres de la `a` a la `z` (minúsculas)\n",
    "- `áéíóú`: Este patrón coincide con todas las vocales con tilde.\n",
    "\n",
    "Todos estos patrones están concatenados con un `O` lógico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalías dentofaciales  incluso la maloclusión     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence_lower_alpha = re.sub(r'[^a-zñáéíóú]', ' ', sample_sentence_lower)\n",
    "sample_sentence_lower_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazamos todas las vocales con tilde con con su forma sin tilde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalías dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('ó', 'o', sample_sentence_lower_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamos todo en una función que normalizará una cadena de texto que le pasemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text, remove_tildes = True):\n",
    "    \"\"\"Normaliza una cadena de texto convirtiéndo todo a minúsculas, quitando los caracteres no alfabéticos y los tildes\"\"\"\n",
    "    text = text.lower() # Llevamos todo a minúscula\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) # Reemplazamos los caracteres no alfabéticos por un espacio\n",
    "    if remove_tildes:\n",
    "        text = re.sub('á', 'a', text) # Reemplazamos los tildes\n",
    "        text = re.sub('é', 'e', text)\n",
    "        text = re.sub('í', 'i', text)\n",
    "        text = re.sub('ó', 'o', text)\n",
    "        text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos del tipo Dataset implementan un método Dataset.map() con el cual podemo aplicar uan función a cada una de las instancias de nuesto conjunto de datos. Lo interesante de este método es que aplica la función de manera paralela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2395fc88a1924c6480a5421507ddba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics_normalized = spanish_diagnostics[\"train\"].map(\n",
    "    lambda x: { # Utilizamos una función anónima que devuelve un diccionario\n",
    "        \"normalized_text\" : normalize(x[\"text\"]) # Esta es una nueva característica que agregaremos a nuestro conjunto de datos.\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nuestro conjunto de datos cuenta con una nueva característica `normalized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'normalized_text': '  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar',\n",
       " 'text': '- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "\n",
    "La tokenización es el proceso de demarcación de secciones de una cadena de caracteres. Estas secciones podrían ser oraciones, palabras o subpalabras.\n",
    "\n",
    "El método más simple para tokenizar una cadena de caracteres en nuestro lenguaje es la separación por espacios. Aplicamos una separación por espacios mediante el método `str.split()` sobre nuestro conjunto de datos normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"normalized_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anomalias',\n",
       " 'dentofaciales',\n",
       " 'incluso',\n",
       " 'la',\n",
       " 'maloclusion',\n",
       " 'discrepancia',\n",
       " 'dentomaxilar']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"normalized_text\"].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien el método de separación por espacios funciona bien en nuestro conjunto de datos normalizado, también quisiéramos tokenizar nuestro texto sin normalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'ANOMALÍAS',\n",
       " 'DENTOFACIALES',\n",
       " '(INCLUSO',\n",
       " 'LA',\n",
       " 'MALOCLUSIÓN)',\n",
       " 'DISCREPANCIA',\n",
       " 'DENTOMAXILAR']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar el mismo métodos podemos observar que no funciona totalmente bien debido a la presencia de caracteres no alfabéticos. Para solucionar esto, existen métodos basados en una serie de reglas para solucionar estos problemas. Utilizaremos la implementación de un tokenizador basado en reglas de la biblioteca de procesamiento de lenguaje natural Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = es_core_news_sm.load().tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-,\n",
       " ANOMALÍAS,\n",
       " DENTOFACIALES,\n",
       " (,\n",
       " INCLUSO,\n",
       " LA,\n",
       " MALOCLUSIÓN,\n",
       " ),\n",
       " \n",
       " \n",
       " \n",
       "  ,\n",
       " DISCREPANCIA,\n",
       " DENTOMAXILAR]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_tokenizer(spanish_diagnostics_normalized[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar el tokenizador basado en reglas, podemos tener resultados mucho mejores que los anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗 Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤗 también cuenta con una biblioteca llamada Tokenizers, con la cual podemos construir nuestro tokenizador basado en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el tokenizador con un modelo WordPiece, el cual parte construyendo un vocabulario que incluye todas los caracteres presentes en el conjunto de datos y posteriormente comienza a mezclar caracteres hasta encontrar conjuntos de caracteres que tienen más probabilidad de aparecer juntos que separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.WordPiece())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta biblioteca nos permite añadir pasos de normalización directamente. Replicamos lo mismo que hacemos con nuestra función `normalizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tokenizers.normalizers.Sequence([\n",
    "    tokenizers.normalizers.Lowercase(), # Llevamos todo a minúscula\n",
    "    tokenizers.normalizers.NFD(), # Separamos cada caracter según los elementos que lo componen: á -> (a, ´)\n",
    "    tokenizers.normalizers.StripAccents(), # Eliminamos todos los acentos\n",
    "    tokenizers.normalizers.Replace(tokenizers.Regex(r\"[^a-z ]\"), \" \") # Reemplazamos todos los caracteres no alfabéticos\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(spanish_diagnostics_normalized[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos el normalizador al tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre tokenizamos nuestro conjunto de datos mediante espacio para delimitar el tamaño que puede tener cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el entrenador que entrenará nuestro tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tokenizers.trainers.WordPieceTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el tokenizador sobre nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(spanish_diagnostics_normalized[\"text\"], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el método `Tokenizer.encode()` obtenemos la representación tokenizada de nuesto texto. Esta representación contiene varios atributos, donde los más interesantes son:\n",
    "\n",
    "- `ids`: Contiene nuestro texto representado a través de una lista que contiene los identificadores de cada token.\n",
    "- `tokens`: Contiene nuestro texto representado a través de una lista que contiene el texto de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_output = tokenizer.encode(spanish_diagnostics_normalized[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[383, 618, 635, 119, 466, 1765, 970]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anomalias',\n",
       " 'dentofaciales',\n",
       " 'incluso',\n",
       " 'la',\n",
       " 'maloclusion',\n",
       " 'discrepancia',\n",
       " 'dentomaxilar']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como lo hicimos anteriormente podemos aplicar paralelamente nuestro tokenizador sobre el conjunto de datos mediante el método `Dataset.map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18458142d13247598c70855572887be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics_normalized_tokenized = spanish_diagnostics_normalized.map(lambda x: {\"tokenized_text\":tokenizer.encode(x[\"text\"]).tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro conjunto de datos ahora contiene el texto tokenizado en la característica `tokenized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'normalized_text': '  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar',\n",
       " 'text': '- ANOMALÍAS DENTOFACIALES (INCLUSO LA MALOCLUSIÓN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'tokenized_text': ['anomalias',\n",
       "  'dentofaciales',\n",
       "  'incluso',\n",
       "  'la',\n",
       "  'maloclusion',\n",
       "  'discrepancia',\n",
       "  'dentomaxilar']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de disminuir la cantidad de características de las representaciones de texto existen métodos que reducen el tamaño de vocabulario al eliminar inflexiones que puedan tener las palabras. Estos métodos son:\n",
    "\n",
    "- Lematización: Este método lleva una palabra en su forma flexionada a su forma base, por ejemplo *tratada* -> *tratar*\n",
    "- Stemming: Este método trunca las palabras de entrada mediante un algoritmo predefinido para encontrar la raíz de la misma, por ejemplo *tratada* -> *trat*\n",
    "\n",
    "El proceso de lematización lo haremos a través de la biblioteca Spacy y el proceso de stemming a través de la biblioteca NLTK utilizando el algoritmo Snowball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el analizador de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos como tokenizador el que entrenamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    return spacy.tokens.Doc(nlp.vocab,tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = custom_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar cómo funcionan estos métodos sobre un texto de prueba de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: pieza\n",
      "Lema: pieza\n",
      "Stem: piez\n",
      "---\n",
      "Token: n\n",
      "Lema: n\n",
      "Stem: n\n",
      "---\n",
      "Token: tratada\n",
      "Lema: tratar\n",
      "Stem: trat\n",
      "---\n",
      "Token: endodonticamente\n",
      "Lema: endodonticamente\n",
      "Stem: endodont\n",
      "---\n",
      "Token: restaurada\n",
      "Lema: restaurar\n",
      "Stem: restaur\n",
      "---\n",
      "Token: con\n",
      "Lema: con\n",
      "Stem: con\n",
      "---\n",
      "Token: ionomero\n",
      "Lema: ionomero\n",
      "Stem: ionomer\n",
      "---\n",
      "Token: y\n",
      "Lema: y\n",
      "Stem: y\n",
      "---\n",
      "Token: resina\n",
      "Lema: resinar\n",
      "Stem: resin\n",
      "---\n",
      "Token: compuesta\n",
      "Lema: componer\n",
      "Stem: compuest\n",
      "---\n",
      "Token: necesita\n",
      "Lema: necesitar\n",
      "Stem: necesit\n",
      "---\n",
      "Token: protesis\n",
      "Lema: protesis\n",
      "Stem: protesis\n",
      "---\n",
      "Token: fija\n",
      "Lema: fijo\n",
      "Stem: fij\n",
      "---\n",
      "Token: por\n",
      "Lema: por\n",
      "Stem: por\n",
      "---\n",
      "Token: gran\n",
      "Lema: gran\n",
      "Stem: gran\n",
      "---\n",
      "Token: pnrdida\n",
      "Lema: pnrdida\n",
      "Stem: pnrdid\n",
      "---\n",
      "Token: coronaria\n",
      "Lema: coronario\n",
      "Stem: coronari\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for t in nlp(spanish_diagnostics_normalized_tokenized[5][\"text\"]):\n",
    "    print(f\"Token: {t.text}\\nLema: {t.lemma_}\\nStem: {stemmer.stem(t.text)}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
