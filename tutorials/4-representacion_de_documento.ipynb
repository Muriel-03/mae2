{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction # Módulo de sklearn para extracción de características\n",
    "import datasets # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
    "import numpy as np # Biblioteca de manejo de datos vectoriales\n",
    "import pandas as pd # Biblioteca de manejo de conjuntos de datos\n",
    "import spacy.lang.es # Biblioteca de procesamiento de lenguaje natural\n",
    "import matplotlib.pyplot as plt # Biblioteca de visualización\n",
    "import sklearn.pipeline # Módulo de sklearn para el desarrollo de flujos de trabajo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset spanish_diagnostics (/home/vscode/.cache/huggingface/datasets/fvillena___spanish_diagnostics/default/0.0.0/45c176cea64580ea9631f78c2867a657ede368597681e5337e9f1c976e4e84ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca3799103434fbca08a0c81840f8b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spanish_diagnostics = datasets.load_dataset('fvillena/spanish_diagnostics') # Cargamos las particiones de entrenamiento y prueba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos una lista de stopwords desde la biblioteca Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.es.stop_words.STOP_WORDS # La biblioteca Spacy tiene una lista de stopwords en español"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de documentos\n",
    "\n",
    "Para poder trabajar con datos de texto, estos deben ser representados de una manera que pueda ser interpretada por los algoritmos de minería de texto. Típicamente se desea llegar a una matriz que tenga tantas filas como documentos tenga nuestro corpus y tantas columnas como características fueron extraídas desde el texto.\n",
    "\n",
    "Revisaremos 2 métodos de extracción de características:\n",
    "\n",
    "* Bag-of-words: Este método extrae la frecuencia de aparición de cada una de las palabras del documento y representa un documento como un vector de tantas dimensiones como palabras tenga el vocabulario.\n",
    "\n",
    "* Term frequency - inverse document frequency (TF-IDF): Este método extrae la frecuencia de aparición de cada una de las palabras y la multiplica por el inverso de la frecuencia de aparición de la palabra en todos los documentos. También se representa cada documento como un vector de tantas dimensiones como palabras tenga el vocabulario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "es una técnica utilizada en el procesamiento del lenguaje natural para representar textos como conjuntos no ordenados de palabras, ignorando la estructura gramatical y el orden de las palabras en el texto. En este enfoque, cada documento se representa mediante un vector que cuenta la ocurrencia o frecuencia de las palabras en el documento.\n",
    "\n",
    "El proceso de creación de una representación de Bolsa de Palabras consta de los siguientes pasos:\n",
    "\n",
    "* Tokenización: El texto se divide en unidades más pequeñas, generalmente palabras. Cada palabra se considera un \"token\".\n",
    "\n",
    "* Creación del vocabulario: Se construye un vocabulario único a partir de todos los tokens en el conjunto de documentos. Cada palabra del vocabulario se asigna a un índice único.\n",
    "\n",
    "* Creación del vector de características: Para cada documento, se crea un vector de características que representa la frecuencia de cada palabra del vocabulario en el documento. El valor en cada posición del vector corresponde a la frecuencia de esa palabra en el documento.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLvya7MXjsSc3NS2SoLMEg.png\" alt=\"medium\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un extractor de características Bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words = list(stopwords), # Le pasamos la lista de stopwords para eliminarlas del vocabulario\n",
    "    max_df = 0.05, # Eliminamos del vocabulario el 5% de palabras más frecuentes (stopwords específicas del corpus)\n",
    "    min_df = 2 # Eliminamos del vocabulario las palabras que tienen una frecuencia menor a 2 (típicamente palabras malformadas)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el vectorizador sobre los textos del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=[&#x27;mias&#x27;, &#x27;nunca&#x27;, &#x27;aun&#x27;, &#x27;estaba&#x27;, &#x27;tres&#x27;,\n",
       "                            &#x27;queremos&#x27;, &#x27;va&#x27;, &#x27;algunos&#x27;, &#x27;nueva&#x27;, &#x27;más&#x27;, &#x27;hoy&#x27;,\n",
       "                            &#x27;estamos&#x27;, &#x27;entonces&#x27;, &#x27;otros&#x27;, &#x27;mayor&#x27;, &#x27;haces&#x27;,\n",
       "                            &#x27;haceis&#x27;, &#x27;junto&#x27;, &#x27;largo&#x27;, &#x27;añadió&#x27;, &#x27;mías&#x27;,\n",
       "                            &#x27;cierta&#x27;, &#x27;alrededor&#x27;, &#x27;ti&#x27;, &#x27;quiza&#x27;, &#x27;vuestra&#x27;,\n",
       "                            &#x27;eras&#x27;, &#x27;una&#x27;, &#x27;esas&#x27;, &#x27;propio&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=[&#x27;mias&#x27;, &#x27;nunca&#x27;, &#x27;aun&#x27;, &#x27;estaba&#x27;, &#x27;tres&#x27;,\n",
       "                            &#x27;queremos&#x27;, &#x27;va&#x27;, &#x27;algunos&#x27;, &#x27;nueva&#x27;, &#x27;más&#x27;, &#x27;hoy&#x27;,\n",
       "                            &#x27;estamos&#x27;, &#x27;entonces&#x27;, &#x27;otros&#x27;, &#x27;mayor&#x27;, &#x27;haces&#x27;,\n",
       "                            &#x27;haceis&#x27;, &#x27;junto&#x27;, &#x27;largo&#x27;, &#x27;añadió&#x27;, &#x27;mías&#x27;,\n",
       "                            &#x27;cierta&#x27;, &#x27;alrededor&#x27;, &#x27;ti&#x27;, &#x27;quiza&#x27;, &#x27;vuestra&#x27;,\n",
       "                            &#x27;eras&#x27;, &#x27;una&#x27;, &#x27;esas&#x27;, &#x27;propio&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=['mias', 'nunca', 'aun', 'estaba', 'tres',\n",
       "                            'queremos', 'va', 'algunos', 'nueva', 'más', 'hoy',\n",
       "                            'estamos', 'entonces', 'otros', 'mayor', 'haces',\n",
       "                            'haceis', 'junto', 'largo', 'añadió', 'mías',\n",
       "                            'cierta', 'alrededor', 'ti', 'quiza', 'vuestra',\n",
       "                            'eras', 'una', 'esas', 'propio', ...])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(spanish_diagnostics[\"train\"][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploraremos cómo está representando nuestros documentos este vectorizador.\n",
    "\n",
    "Este es un texto de ejemplo del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized = count_vectorizer.transform(spanish_diagnostics[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 13565)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- CARIES DENTINARIA PROFUNDA/  - Fundamento Clínico APS: caries profunda en relacion de la pieza dental 4.5 Caries de la dentina'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][\"text\"][69983]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([spanish_diagnostics[\"train\"][\"text\"][69983]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_scores(text,vectorizer):\n",
    "    \"\"\"A partir de un texto y un vectorizador retorna los puntajes asignados a cada palabra del texto\"\"\"\n",
    "    feature_names = list({k: v for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}.keys()) # Vocabulario\n",
    "    doc = vectorizer.transform([text]) # Vectorizamos el texto de entrada\n",
    "    idxs = np.argwhere(doc)[:,1] # Extraemos los índices donde sí hay palabras representadas\n",
    "    words = [feature_names[i] for i in idxs] # Extraemos las palabras asociadas a los índices extraídos\n",
    "    scores = np.array(doc.todense())[0][idxs] # Extraemos los puntajes asociadas a los índices extraídos\n",
    "    return list(reversed(sorted(zip(words,scores),key=lambda tup: tup[1]))) # Retornamos una lista de palabras y puntajes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nuestro vectorizador le dio más peso a la palabra caries de nuestro documento porque es la palabra más frecuente y a a un grupo de palabras les asignó el mismo puntaje 1 porque cada una aprece 1 vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caries', 3),\n",
       " ('profunda', 2),\n",
       " ('relacion', 1),\n",
       " ('fundamento', 1),\n",
       " ('dentinaria', 1),\n",
       " ('dentina', 1),\n",
       " ('dental', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],count_vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Es una técnica utilizada en el procesamiento del lenguaje natural para ponderar la importancia de un término (palabra) en un documento dentro de una colección de documentos. Consiste en dos componentes principales: TF (Frecuencia del Término) e IDF (Frecuencia Inversa del Documento).\n",
    "\n",
    "* Frecuencia del Término (TF): La componente TF mide la frecuencia relativa de un término en un documento específico. El objetivo es asignar un mayor peso a los términos que aparecen con más frecuencia dentro del documento, ya que se considera que estos términos son más importantes. \n",
    "La fórmula básica para calcular la TF es: $$TF = \\frac{(\\text{Número de veces que aparece el término en el documento})}{(\\text{Número total de términos en el documento})}$$\n",
    "\n",
    "    Ejemplo de TF: Supongamos que tenemos un documento que contiene la siguiente frase: \"El perro juega en el parque\". Si queremos calcular la frecuencia del término \"perro\" en este documento, contaríamos que aparece una vez. Si asumimos que hay un total de cinco términos en el documento, entonces la frecuencia del término \"perro\" sería $\\frac{1}{5}$ = 0.2.\n",
    "\n",
    "\n",
    "* Frecuencia Inversa del Documento (IDF): La componente IDF mide la importancia de un término en el contexto de una colección de documentos. Se basa en la suposición de que los términos menos comunes en la colección pueden ser más informativos que los términos comunes. La fórmula básica para calcular el IDF es: $$IDF = log(\\frac{\\text{Número total de documentos en la colección}}{\\text{Número de documentos que contienen el término}})$$\n",
    "\n",
    "    El IDF se calcula como el logaritmo del cociente entre el número total de documentos y el número de documentos que contienen el término dado. El logaritmo se aplica para suavizar la escala del IDF y evitar una sobrevaloración de términos muy raros.\n",
    "\n",
    "    Ejemplo de IDF: Supongamos que tenemos una colección de documentos que contiene un total de 100 documentos. Si el término \"perro\" aparece en 50 de esos documentos, entonces el IDF se calcularía como log(100/50) = log(2) ≈ 0.301.\n",
    "\n",
    "\n",
    "* TF-IDF: El TF-IDF combina la información de TF e IDF para calcular un peso final para cada término en un documento. Se calcula multiplicando la TF del término en el documento por su IDF en la colección. La fórmula para calcular el TF-IDF es: $$TF-IDF = TF * IDF$$\n",
    "\n",
    "    El TF-IDF aumentará para los términos que aparezcan con frecuencia en un documento específico (alta TF) y sean menos comunes en la colección en su conjunto (alta IDF).\n",
    "\n",
    "    Ejemplo de TF-IDF: Supongamos que queremos calcular el TF-IDF para el término \"perro\" en un documento específico. Si el valor de TF es 0.2 (como en el ejemplo anterior) y el valor de IDF es 0.301 (como en el ejemplo IDF), entonces el valor de TF-IDF sería 0.2 * 0.301 = 0.0602."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos un vectorizador que utiliza TF-IDF y lo ajustamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=[&#x27;mias&#x27;, &#x27;nunca&#x27;, &#x27;aun&#x27;, &#x27;estaba&#x27;, &#x27;tres&#x27;,\n",
       "                            &#x27;queremos&#x27;, &#x27;va&#x27;, &#x27;algunos&#x27;, &#x27;nueva&#x27;, &#x27;más&#x27;, &#x27;hoy&#x27;,\n",
       "                            &#x27;estamos&#x27;, &#x27;entonces&#x27;, &#x27;otros&#x27;, &#x27;mayor&#x27;, &#x27;haces&#x27;,\n",
       "                            &#x27;haceis&#x27;, &#x27;junto&#x27;, &#x27;largo&#x27;, &#x27;añadió&#x27;, &#x27;mías&#x27;,\n",
       "                            &#x27;cierta&#x27;, &#x27;alrededor&#x27;, &#x27;ti&#x27;, &#x27;quiza&#x27;, &#x27;vuestra&#x27;,\n",
       "                            &#x27;eras&#x27;, &#x27;una&#x27;, &#x27;esas&#x27;, &#x27;propio&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=[&#x27;mias&#x27;, &#x27;nunca&#x27;, &#x27;aun&#x27;, &#x27;estaba&#x27;, &#x27;tres&#x27;,\n",
       "                            &#x27;queremos&#x27;, &#x27;va&#x27;, &#x27;algunos&#x27;, &#x27;nueva&#x27;, &#x27;más&#x27;, &#x27;hoy&#x27;,\n",
       "                            &#x27;estamos&#x27;, &#x27;entonces&#x27;, &#x27;otros&#x27;, &#x27;mayor&#x27;, &#x27;haces&#x27;,\n",
       "                            &#x27;haceis&#x27;, &#x27;junto&#x27;, &#x27;largo&#x27;, &#x27;añadió&#x27;, &#x27;mías&#x27;,\n",
       "                            &#x27;cierta&#x27;, &#x27;alrededor&#x27;, &#x27;ti&#x27;, &#x27;quiza&#x27;, &#x27;vuestra&#x27;,\n",
       "                            &#x27;eras&#x27;, &#x27;una&#x27;, &#x27;esas&#x27;, &#x27;propio&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_df=0.05, min_df=2,\n",
       "                stop_words=['mias', 'nunca', 'aun', 'estaba', 'tres',\n",
       "                            'queremos', 'va', 'algunos', 'nueva', 'más', 'hoy',\n",
       "                            'estamos', 'entonces', 'otros', 'mayor', 'haces',\n",
       "                            'haceis', 'junto', 'largo', 'añadió', 'mías',\n",
       "                            'cierta', 'alrededor', 'ti', 'quiza', 'vuestra',\n",
       "                            'eras', 'una', 'esas', 'propio', ...])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words = list(stopwords),\n",
    "    max_df = 0.05,\n",
    "    min_df = 2\n",
    ")\n",
    "tfidf_vectorizer.fit(spanish_diagnostics[\"train\"][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que caries sigue siendo la palabra con el mayor puntaje. Pero vemos que todas las palabras tienen un puntaje distinto, en donde destacamos que la palabra fundamento tiene el menor puntaje. Intuitivamente podemos darnos cuenta que esta representación es mejor porque la palabra fundamento no nos aporta mucha información en el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caries', 0.5841232685802328),\n",
       " ('profunda', 0.5052614123771014),\n",
       " ('dentina', 0.36362077674894117),\n",
       " ('relacion', 0.30054370920913476),\n",
       " ('dentinaria', 0.27911008594273823),\n",
       " ('dental', 0.25831112531154554),\n",
       " ('fundamento', 0.19062330472026245)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],tfidf_vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizamos los textos de nuestros conjuntos de entrenamiento y prueba. con el método CountVectorizer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized_train = count_vectorizer.transform(spanish_diagnostics[\"train\"][\"text\"])\n",
    "text_vectorized_test = count_vectorizer.transform(spanish_diagnostics[\"test\"][\"text\"])\n",
    "feature_names = list({k: v for k, v in sorted(count_vectorizer.vocabulary_.items(), key=lambda item: item[1])}.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de nuestra matriz es de (cantidad de documentos en el conjunto, tamaño del vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 13565)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 13565)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoiler: Pipelines de Sklearn\n",
    "\n",
    "Los pipelines en la biblioteca scikit-learn son una forma conveniente y eficiente de encadenar múltiples pasos de procesamiento de datos y modelos de aprendizaje automático en un flujo de trabajo coherente. Un pipeline de scikit-learn combina transformadores y estimadores en una secuencia ordenada, donde los datos se pasan secuencialmente a través de cada etapa para su procesamiento.\n",
    "\n",
    "En scikit-learn, un transformador es cualquier objeto que implementa los métodos fit() y transform(). Los transformadores se utilizan para realizar transformaciones en los datos, como preprocesamiento, extracción de características o reducción de dimensionalidad. Por otro lado, un estimador es cualquier objeto que implementa los métodos fit() y predict(). Los estimadores se utilizan para ajustar modelos a los datos y realizar predicciones.\n",
    "\n",
    "El pipeline de scikit-learn permite definir una secuencia de pasos, donde cada paso es un par formado por un nombre y un transformador o estimador. Los datos se pasan a través de los pasos en el orden especificado, y cada paso toma los datos de entrada, realiza su operación y pasa los datos transformados al siguiente paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words = list(stopwords), # Le pasamos la lista de stopwords para eliminarlas del vocabulario\n",
    "    max_df = 0.05, # Eliminamos del vocabulario el 5% de palabras más frecuentes (stopwords específicas del corpus)\n",
    "    min_df = 2 # Eliminamos del vocabulario las palabras que tienen una frecuencia menor a 2 (típicamente palabras malformadas)\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words = list(stopwords),\n",
    "    max_df = 0.05,\n",
    "    min_df = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = sklearn.pipeline.Pipeline([\n",
    "        ('vectorizer', count_vectorizer),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x13565 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 372583 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.fit_transform(spanish_diagnostics[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caries', 3),\n",
       " ('profunda', 2),\n",
       " ('relacion', 1),\n",
       " ('fundamento', 1),\n",
       " ('dentinaria', 1),\n",
       " ('dentina', 1),\n",
       " ('dental', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],pipe_bow.named_steps['vectorizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tf_idf = sklearn.pipeline.Pipeline([\n",
    "        ('vectorizer', tfidf_vectorizer),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x13565 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 372583 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tf_idf.fit_transform(spanish_diagnostics[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caries', 0.5841232685802328),\n",
       " ('profunda', 0.5052614123771014),\n",
       " ('dentina', 0.36362077674894117),\n",
       " ('relacion', 0.30054370920913476),\n",
       " ('dentinaria', 0.27911008594273823),\n",
       " ('dental', 0.25831112531154554),\n",
       " ('fundamento', 0.19062330472026245)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],pipe_tf_idf.named_steps['vectorizer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿se puede meter mas pasos a un pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pipe = sklearn.pipeline.Pipeline([\n",
    "        ('vectorizer', vectorizer), #preprocesamiento\n",
    "        ('classifier', sklearn.naive_bayes.MultinomialNB()) #clasificador\n",
    "    ])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
